# -*- coding: utf-8 -*-
"""summarizer.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1bGZH1gHxyG7AYvqNPNIsSPTgHaebq2hG
"""

import spacy
from spacy.lang.en.stop_words import STOP_WORDS
from string import punctuation
from heapq import nlargest

def summarizer(raw_docs):
    # Importing the list of stopwords from spaCy's language processing module
    stopwords = list(STOP_WORDS)

    # Loading the English language model from spaCy
    nlp = spacy.load('en_core_web_sm')

    # Using the loaded language model to process the input text
    doc = nlp(raw_docs)

    # Printing the processed document
    #print(doc)

    # Extracting tokens from the processed document
    tokens = [token.text for token in doc]

    # Printing the list of extracted tokens
    #print(tokens)

    # Initializing an empty dictionary to store word frequencies
    word_frequency = {}

    # Iterate through each word in the document
    for word in doc:
        # Check if the lowercase version of the word is not in stopwords and not in punctuation
        if word.text.lower() not in stopwords and word.text.lower() not in punctuation:
            # Check if the word is not already in the dictionary
            if word.text not in word_frequency.keys():
                # If not, add the word to the dictionary with a frequency of 1
                word_frequency[word.text] = 1
            else:
                # If the word is already in the dictionary, increment its frequency count by 1
                word_frequency[word.text] += 1

    # Print the word frequencies dictionary
   # print(word_frequency)

    # Finding the maximum frequency value in the word frequency dictionary
    max_frequency = max(word_frequency.values())

    # Printing the maximum frequency value
    #print(max_frequency)

    # Normalize the frequency of each word by dividing its frequency by the maximum frequency
    # This helps in making the frequencies comparable across different texts/documents
    # max_frequency represents the highest frequency of any word in the entire dictionary
    for word in word_frequency.keys():
        word_frequency[word] = word_frequency[word] / max_frequency

    # Extracting sentences from the processed document
    sent_tokens = [sent for sent in doc.sents]
    #print(sent_tokens)

    # Initializing an empty dictionary to store sentence scores
    sent_score = {}

    # Iterate over each sentence in the list of sentence tokens
    for sent in sent_tokens:
        for word in sent:
            if word.text.lower() in word_frequency.keys():
                if sent not in sent_score.keys():
                    sent_score[sent] = word_frequency[word.text.lower()]
                else:
                    sent_score[sent] += word_frequency[word.text.lower()]

    # Printing the sentence scores dictionary
    #print(sent_score)

    # Selecting the top sentences with the highest scores
    select_len = int(len(sent_tokens) * 0.3)
    #print(select_len)

    # Selecting the top sentences with the highest scores
    summary = nlargest(select_len, sent_score, key=sent_score.get)

    # Printing the summary
    #print(summary)

    # Printing the lengths of the original and summary texts
    #print("The length of original text:", len(raw_docs.split(' ')))
    #print("The length of summary text:", len(' '.join([sent.text for sent in summary]).split(' ')))

    return summary, doc, len(raw_docs.split(' ')), len(' '.join([sent.text for sent in summary]).split(' '))

# Example usage:
raw_text = """Text summarization is an important aspect of natural language processing due to its ability to process large caches of information for various fields, including journalism, legal, scientific and research.

In this project, we used Flask framework, and SpaCy to build a text summarization web application. The application can summarize texts of varying lengths and return a summary with a reading time estimation. We also included a reset button that allows users to input a new text for summarization.

The project structure includes the Flask application file (app.py), two HTML templates (index.html and result.html), and a static folder that contains the W3.CSS file and a custom style.css file. We used Flask to handle HTTP requests and created HTML templates to render the applicationâ€™s front end.

In conclusion, the text summarization web application we built using Flask, Spacy can summarize large texts quickly. It can be further improved by adding features such as language detection and better summarization algorithms.."""
summarizer(raw_text)